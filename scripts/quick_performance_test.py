#!/usr/bin/env python3
"""
å¿«é€Ÿæ€§èƒ½æµ‹è¯•è„šæœ¬
ç”¨äºå¿«é€Ÿè¯„ä¼°å½“å‰é…ç½®çš„æ€§èƒ½è¡¨ç°
"""

import sys
import os
import time
import json
import statistics
import numpy as np
from datetime import datetime
from PIL import Image
from io import BytesIO

# æ·»åŠ è·¯å¾„
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..")))
sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), "..", "backend")))

from config import config
from backend.model_module.feature_extractor import feature_extractor

def generate_test_image(size=(224, 224), color=None):
    """ç”Ÿæˆå•ä¸ªæµ‹è¯•å›¾åƒ"""
    if color is None:
        color = (
            int(np.random.randint(0, 256)),
            int(np.random.randint(0, 256)),
            int(np.random.randint(0, 256))
        )
    return Image.new('RGB', size, color=color)

def test_current_config():
    """æµ‹è¯•å½“å‰é…ç½®çš„æ€§èƒ½"""
    print("ğŸ”§ å½“å‰é…ç½®ä¿¡æ¯:")
    print(f"  è®¾å¤‡: {config.device}")
    print(f"  æ‰¹å¤„ç†å¤§å°: {config.batchsize}")
    print(f"  ç°‡å¤§å° (N_LIST): {config.N_LIST}")
    print(f"  åˆ†å¸ƒå¼å¯ç”¨: {config.DISTRIBUTED_AVAILABLE}")
    print()
    
    # ç”Ÿæˆæµ‹è¯•å›¾åƒ
    print("ğŸ“¸ ç”Ÿæˆæµ‹è¯•å›¾åƒ...")
    test_images = []
    for i in range(10):
        img = generate_test_image()
        test_images.append(img)
    print(f"âœ… å·²ç”Ÿæˆ {len(test_images)} å¼ æµ‹è¯•å›¾åƒ")
      # æµ‹è¯•ç‰¹å¾æå–æ€§èƒ½
    print("\nâš¡ æµ‹è¯•ç‰¹å¾æå–æ€§èƒ½...")
    
    # å®ä¾‹åŒ–ç‰¹å¾æå–å™¨
    from backend.model_module.feature_extractor import feature_extractor
    extractor = feature_extractor()
    
    # å•å›¾åƒæµ‹è¯•
    print("  å•å›¾åƒå¤„ç†:")
    single_times = []
    for _ in range(5):
        start_time = time.time()
        features = extractor.calculate(test_images[0])
        end_time = time.time()
        single_times.append(end_time - start_time)
    
    avg_single = statistics.mean(single_times)
    print(f"    å¹³å‡æ—¶é—´: {avg_single:.3f}s")
    print(f"    ååé‡: {1.0/avg_single:.2f} å›¾åƒ/ç§’")
      # æ‰¹å¤„ç†æµ‹è¯•
    batch_size = min(config.batchsize, len(test_images))
    print(f"  æ‰¹å¤„ç† ({batch_size} å›¾åƒ):")
    batch_times = []
    for _ in range(3):
        start_time = time.time()
        features = extractor.calculate_batch(test_images[:batch_size])
        end_time = time.time()
        batch_times.append(end_time - start_time)
    
    avg_batch = statistics.mean(batch_times)
    batch_throughput = batch_size / avg_batch
    print(f"    å¹³å‡æ—¶é—´: {avg_batch:.3f}s")
    print(f"    ååé‡: {batch_throughput:.2f} å›¾åƒ/ç§’")
    print(f"    å•å›¾åƒå¹³å‡æ—¶é—´: {avg_batch/batch_size:.3f}s")
    
    # æ•ˆç‡åˆ†æ
    efficiency = (avg_single * batch_size) / avg_batch if avg_batch > 0 else 0
    print(f"    æ‰¹å¤„ç†æ•ˆç‡: {efficiency:.2f}x")
    
    return {
        'single_image': {
            'avg_time': avg_single,
            'throughput': 1.0/avg_single if avg_single > 0 else 0,
            'times': single_times
        },
        'batch_processing': {
            'batch_size': batch_size,
            'avg_time': avg_batch,
            'throughput': batch_throughput,
            'per_image_time': avg_batch/batch_size if batch_size > 0 else 0,
            'efficiency': efficiency,
            'times': batch_times
        }
    }

def test_distributed_availability():
    """æµ‹è¯•åˆ†å¸ƒå¼è®¡ç®—å¯ç”¨æ€§"""
    print("\nğŸŒ æµ‹è¯•åˆ†å¸ƒå¼è®¡ç®—å¯ç”¨æ€§...")
    
    if not config.DISTRIBUTED_AVAILABLE:
        print("âŒ åˆ†å¸ƒå¼è®¡ç®—å·²ç¦ç”¨")
        return False
    
    try:
        # æ£€æŸ¥Redisè¿æ¥
        import redis
        r = redis.Redis(
            host=config.REDIS_HOST, 
            port=config.REDIS_PORT, 
            db=config.REDIS_BROKER_DB
        )
        r.ping()
        print("âœ… Redisè¿æ¥æ­£å¸¸")
        
        # æ£€æŸ¥Celery Worker
        from backend.worker import celery_app
        i = celery_app.control.inspect()
        active_workers = i.active()
        
        if active_workers:
            print(f"âœ… å‘ç° {len(active_workers)} ä¸ªæ´»è·ƒWorker")
            return True
        else:
            print("âŒ æ²¡æœ‰å‘ç°æ´»è·ƒçš„Worker")
            return False
            
    except Exception as e:
        print(f"âŒ åˆ†å¸ƒå¼è®¡ç®—ä¸å¯ç”¨: {e}")
        return False

def compare_local_vs_distributed():
    """æ¯”è¾ƒæœ¬åœ°å’Œåˆ†å¸ƒå¼è®¡ç®—æ€§èƒ½"""
    print("\nâš–ï¸  æ¯”è¾ƒæœ¬åœ° vs åˆ†å¸ƒå¼è®¡ç®—...")
    
    # ç”Ÿæˆæµ‹è¯•å›¾åƒ
    test_img = generate_test_image()
    img_buffer = BytesIO()
    test_img.save(img_buffer, format='JPEG')
    img_data = img_buffer.getvalue()
    import base64
    img_data_b64 = base64.b64encode(img_data).decode('utf-8')
    
    results = {}
      # æœ¬åœ°è®¡ç®—æµ‹è¯•
    print("  ğŸ  æœ¬åœ°è®¡ç®—:")
    original_distributed = config.DISTRIBUTED_AVAILABLE
    config.DISTRIBUTED_AVAILABLE = False
    
    # å®ä¾‹åŒ–ç‰¹å¾æå–å™¨
    from backend.model_module.feature_extractor import feature_extractor
    extractor = feature_extractor()
    
    local_times = []
    for _ in range(3):
        start_time = time.time()
        features = extractor.calculate(test_img)
        end_time = time.time()
        local_times.append(end_time - start_time)
    
    local_avg = statistics.mean(local_times)
    print(f"    å¹³å‡æ—¶é—´: {local_avg:.3f}s")
    results['local'] = {'avg_time': local_avg, 'times': local_times}
    
    # åˆ†å¸ƒå¼è®¡ç®—æµ‹è¯•
    config.DISTRIBUTED_AVAILABLE = original_distributed
    
    if test_distributed_availability():
        print("  â˜ï¸  åˆ†å¸ƒå¼è®¡ç®—:")
        try:
            from backend.worker import generate_embeddings_task
            
            dist_times = []
            for _ in range(3):
                start_time = time.time()
                future = generate_embeddings_task.delay(img_data_b64)
                result = future.get(timeout=30)
                end_time = time.time()
                dist_times.append(end_time - start_time)
            
            dist_avg = statistics.mean(dist_times)
            print(f"    å¹³å‡æ—¶é—´: {dist_avg:.3f}s")
            
            # è®¡ç®—åŠ é€Ÿæ¯”
            speedup = local_avg / dist_avg if dist_avg > 0 else 0
            print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
            
            results['distributed'] = {
                'avg_time': dist_avg, 
                'times': dist_times,
                'speedup': speedup
            }
            
        except Exception as e:
            print(f"    âŒ åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥: {e}")
            results['distributed'] = {'error': str(e)}
    else:
        print("  âŒ åˆ†å¸ƒå¼è®¡ç®—ä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        results['distributed'] = {'error': 'åˆ†å¸ƒå¼è®¡ç®—ä¸å¯ç”¨'}
    
    return results

def test_different_batch_sizes():
    """æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°çš„æ€§èƒ½"""
    print("\nğŸ“Š æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°...")
    
    # ç”Ÿæˆè¶³å¤Ÿçš„æµ‹è¯•å›¾åƒ
    test_images = [generate_test_image() for _ in range(32)]
    batch_sizes = [1, 2, 4, 8, 16]
    results = {}
    
    original_batchsize = config.batchsize
    
    for batch_size in batch_sizes:
        if batch_size > len(test_images):
            continue
            
        print(f"  æ‰¹å¤„ç†å¤§å°: {batch_size}")
        config.batchsize = batch_size
        
        # å®ä¾‹åŒ–ç‰¹å¾æå–å™¨
        from backend.model_module.feature_extractor import feature_extractor
        extractor = feature_extractor()
        
        times = []
        for _ in range(3):
            start_time = time.time()
            features = extractor.calculate_batch(test_images[:batch_size])
            end_time = time.time()
            times.append(end_time - start_time)
        
        avg_time = statistics.mean(times)
        throughput = batch_size / avg_time
        per_image_time = avg_time / batch_size
        
        results[batch_size] = {
            'avg_time': avg_time,
            'throughput': throughput,
            'per_image_time': per_image_time,
            'times': times
        }
        
        print(f"    å¹³å‡æ—¶é—´: {avg_time:.3f}s")
        print(f"    ååé‡: {throughput:.2f} å›¾åƒ/ç§’")
        print(f"    å•å›¾åƒæ—¶é—´: {per_image_time:.3f}s")
    
    # æ¢å¤åŸå§‹é…ç½®
    config.batchsize = original_batchsize
    
    return results

def generate_quick_report(results):
    """ç”Ÿæˆå¿«é€ŸæŠ¥å‘Š"""
    print("\n" + "="*60)
    print("ğŸ“‹ å¿«é€Ÿæ€§èƒ½æŠ¥å‘Š")
    print("="*60)
    
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"æµ‹è¯•æ—¶é—´: {timestamp}")
    print()
    
    # å½“å‰é…ç½®æ€§èƒ½
    if 'current_config' in results:
        current = results['current_config']
        print("ğŸ“Š å½“å‰é…ç½®æ€§èƒ½:")
        print(f"  å•å›¾åƒå¤„ç†: {current['single_image']['avg_time']:.3f}s")
        print(f"  å•å›¾åƒååé‡: {current['single_image']['throughput']:.2f} å›¾åƒ/ç§’")
        print(f"  æ‰¹å¤„ç†æ•ˆç‡: {current['batch_processing']['efficiency']:.2f}x")
        print()
    
    # æœ¬åœ° vs åˆ†å¸ƒå¼å¯¹æ¯”
    if 'local_vs_distributed' in results:
        comparison = results['local_vs_distributed']
        print("âš–ï¸  æœ¬åœ° vs åˆ†å¸ƒå¼å¯¹æ¯”:")
        if 'local' in comparison:
            print(f"  æœ¬åœ°è®¡ç®—: {comparison['local']['avg_time']:.3f}s")
        if 'distributed' in comparison and 'error' not in comparison['distributed']:
            print(f"  åˆ†å¸ƒå¼è®¡ç®—: {comparison['distributed']['avg_time']:.3f}s")
            print(f"  åŠ é€Ÿæ¯”: {comparison['distributed']['speedup']:.2f}x")
        elif 'distributed' in comparison:
            print(f"  åˆ†å¸ƒå¼è®¡ç®—: å¤±è´¥ ({comparison['distributed']['error']})")
        print()
    
    # æ‰¹å¤„ç†å¤§å°æ€§èƒ½
    if 'batch_sizes' in results:
        batch_results = results['batch_sizes']
        print("ğŸ“ˆ æ‰¹å¤„ç†å¤§å°æ€§èƒ½:")
        print("  å¤§å° | æ—¶é—´(s) | ååé‡(å›¾åƒ/s) | å•å›¾åƒæ—¶é—´(s)")
        print("  -----|---------|---------------|---------------")
        for batch_size, metrics in batch_results.items():
            print(f"  {batch_size:4d} | {metrics['avg_time']:7.3f} | {metrics['throughput']:13.2f} | {metrics['per_image_time']:13.3f}")
        print()
    
    # å»ºè®®
    print("ğŸ’¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®:")
    
    if 'batch_sizes' in results:
        # æ‰¾åˆ°æœ€ä½³æ‰¹å¤„ç†å¤§å°
        best_batch = max(results['batch_sizes'].items(), 
                        key=lambda x: x[1]['throughput'])
        print(f"  â€¢ æ¨èæ‰¹å¤„ç†å¤§å°: {best_batch[0]} (ååé‡: {best_batch[1]['throughput']:.2f} å›¾åƒ/ç§’)")
    
    if 'local_vs_distributed' in results:
        comparison = results['local_vs_distributed']
        if 'distributed' in comparison and 'speedup' in comparison['distributed']:
            speedup = comparison['distributed']['speedup']
            if speedup > 1.2:
                print(f"  â€¢ å»ºè®®ä½¿ç”¨åˆ†å¸ƒå¼è®¡ç®— (åŠ é€Ÿæ¯”: {speedup:.2f}x)")
            elif speedup < 0.8:
                print("  â€¢ å»ºè®®ä½¿ç”¨æœ¬åœ°è®¡ç®— (åˆ†å¸ƒå¼è®¡ç®—è¾ƒæ…¢)")
            else:
                print("  â€¢ æœ¬åœ°å’Œåˆ†å¸ƒå¼è®¡ç®—æ€§èƒ½ç›¸è¿‘ï¼Œå¯æ ¹æ®éœ€è¦é€‰æ‹©")
        else:
            print("  â€¢ æ£€æŸ¥åˆ†å¸ƒå¼è®¡ç®—é…ç½®ï¼Œç¡®ä¿Workeræ­£å¸¸è¿è¡Œ")
    
    # æ£€æŸ¥é…ç½®åˆç†æ€§
    current_config = results.get('current_config', {})
    if current_config:
        single_throughput = current_config['single_image']['throughput']
        if single_throughput < 1.0:
            print("  â€¢ è€ƒè™‘ä½¿ç”¨æ›´å¼ºçš„ç¡¬ä»¶æˆ–ä¼˜åŒ–æ¨¡å‹é…ç½®")
        
        efficiency = current_config['batch_processing']['efficiency']
        if efficiency < 2.0:
            print("  â€¢ è€ƒè™‘è°ƒæ•´æ‰¹å¤„ç†å¤§å°ä»¥æé«˜æ•ˆç‡")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
    print("="*60)
    
    results = {}
    
    # æµ‹è¯•å½“å‰é…ç½®
    results['current_config'] = test_current_config()
    
    # æµ‹è¯•åˆ†å¸ƒå¼è®¡ç®—å¯ç”¨æ€§å¹¶æ¯”è¾ƒæ€§èƒ½
    results['local_vs_distributed'] = compare_local_vs_distributed()
    
    # æµ‹è¯•ä¸åŒæ‰¹å¤„ç†å¤§å°
    results['batch_sizes'] = test_different_batch_sizes()
    
    # ç”ŸæˆæŠ¥å‘Š
    generate_quick_report(results)
      # ä¿å­˜ç»“æœ
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    os.makedirs("../performance_reports", exist_ok=True)
    
    with open(f"../performance_reports/quick_test_{timestamp}.json", "w", encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False)
    
    print(f"\nâœ… æµ‹è¯•å®Œæˆï¼Œç»“æœå·²ä¿å­˜åˆ° performance_reports/quick_test_{timestamp}.json")

if __name__ == "__main__":
    main()
