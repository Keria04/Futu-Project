#!/usr/bin/env python3
"""
æ€§èƒ½æµ‹è¯•åŸºå‡†å·¥å…· (Performance Testbench)
ç”¨äºè¯„ä¼°è¿œç¨‹å’Œæœ¬åœ°ä¸¤ç§è®¡ç®—æ–¹å¼çš„æ€§èƒ½ï¼Œä»¥åŠåˆ†æå„ç§é…ç½®å‚æ•°å¯¹æ€§èƒ½çš„å½±å“

ä¸»è¦æµ‹è¯•é¡¹ç›®ï¼š
1. è¿œç¨‹ï¼ˆCeleryåˆ†å¸ƒå¼ï¼‰vs æœ¬åœ°è®¡ç®—æ€§èƒ½å¯¹æ¯”
2. ç°‡å¤§å°ï¼ˆN_LISTï¼‰å¯¹æ£€ç´¢æ€§èƒ½çš„å½±å“
3. æ‰¹å¤„ç†å¤§å°ï¼ˆbatchsizeï¼‰å¯¹ç‰¹å¾æå–æ€§èƒ½çš„å½±å“
4. è®¾å¤‡ç±»å‹ï¼ˆCPU vs GPUï¼‰å¯¹æ€§èƒ½çš„å½±å“
5. ä¸åŒæ•°æ®é›†å¤§å°å¯¹æ€§èƒ½çš„å½±å“
"""

import sys
import os
import time
import json
import base64
import statistics
import numpy as np
from datetime import datetime
from PIL import Image
from io import BytesIO
from concurrent.futures import ThreadPoolExecutor, as_completed
import matplotlib.pyplot as plt
import seaborn as sns
from tabulate import tabulate

# æ·»åŠ è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), ".."))
sys.path.insert(0, project_root)

# å¯¼å…¥é¡¹ç›®æ¨¡å—
from config import config
from backend.model_module.feature_extractor import feature_extractor
from backend.faiss_module.search_index import search_index
from backend.database_module.query import query_multi
from backend.search_module.search import search_image

class PerformanceTestbench:
    """æ€§èƒ½æµ‹è¯•åŸºå‡†ç±»"""
    
    def __init__(self):
        self.results = {}
        self.test_images = []
        self.original_config = {}
        self.backup_config()
        
    def backup_config(self):
        """å¤‡ä»½åŸå§‹é…ç½®"""
        self.original_config = {
            'device': config.device,
            'batchsize': config.batchsize,
            'N_LIST': config.N_LIST,
            'DISTRIBUTED_AVAILABLE': config.DISTRIBUTED_AVAILABLE,
            'CELERY_TASK_TIME_LIMIT': config.CELERY_TASK_TIME_LIMIT,
            'CELERY_TASK_SOFT_TIME_LIMIT': config.CELERY_TASK_SOFT_TIME_LIMIT
        }
        
    def restore_config(self):
        """æ¢å¤åŸå§‹é…ç½®"""
        for key, value in self.original_config.items():
            setattr(config, key, value)
            
    def generate_test_images(self, count=20, size=(224, 224)):
        """ç”Ÿæˆæµ‹è¯•å›¾åƒ"""
        print(f"ç”Ÿæˆ {count} å¼ æµ‹è¯•å›¾åƒ...")
        self.test_images = []
        
        for i in range(count):
            # ç”Ÿæˆéšæœºé¢œè‰²çš„å›¾åƒ
            color = (
                int(np.random.randint(0, 256)),
                int(np.random.randint(0, 256)),
                int(np.random.randint(0, 256))
            )
            img = Image.new('RGB', size, color=color)
            
            # æ·»åŠ ä¸€äº›ç®€å•çš„æ¨¡å¼
            if i % 4 == 0:  # æ·»åŠ åœ†å½¢
                from PIL import ImageDraw
                draw = ImageDraw.Draw(img)
                draw.ellipse([50, 50, 174, 174], fill=(255, 255, 255))
            elif i % 4 == 1:  # æ·»åŠ çŸ©å½¢
                from PIL import ImageDraw
                draw = ImageDraw.Draw(img)
                draw.rectangle([50, 50, 174, 174], fill=(255, 255, 255))
            
            # è½¬æ¢ä¸ºbase64
            img_buffer = BytesIO()
            img.save(img_buffer, format='JPEG')
            img_data = img_buffer.getvalue()
            img_data_b64 = base64.b64encode(img_data).decode('utf-8')
            
            self.test_images.append({
                'id': i,
                'image': img,
                'base64': img_data_b64,
                'size': len(img_data)
            })
            
        print(f"âœ… å·²ç”Ÿæˆ {len(self.test_images)} å¼ æµ‹è¯•å›¾åƒ")
        
    def test_feature_extraction_performance(self):
        """æµ‹è¯•ç‰¹å¾æå–æ€§èƒ½"""
        print("\n" + "="*80)
        print("ğŸ“Š ç‰¹å¾æå–æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        batch_sizes = [1, 4, 8, 16, 32, 64, 128]
        devices = ['cpu']
        
        # å¦‚æœæ”¯æŒGPUï¼Œæ·»åŠ GPUæµ‹è¯•
        try:
            import torch
            if torch.cuda.is_available():
                devices.append('cuda')
        except ImportError:
            pass
            
        results = {}
        
        for device in devices:
            print(f"\nğŸ”§ æµ‹è¯•è®¾å¤‡: {device}")
            results[device] = {}
            
            # è®¾ç½®è®¾å¤‡
            original_device = config.device
            config.device = device
            
            for batch_size in batch_sizes:
                if batch_size > len(self.test_images):
                    continue
                    
                print(f"  æ‰¹å¤„ç†å¤§å°: {batch_size}")
                config.batchsize = batch_size
                
                # å‡†å¤‡æµ‹è¯•å›¾åƒ
                test_batch = [img['image'] for img in self.test_images[:batch_size]]
                
                # æ‰§è¡Œå¤šæ¬¡æµ‹è¯•å–å¹³å‡å€¼
                times = []
                for _ in range(3):
                    start_time = time.time()
                    features = feature_extractor(test_batch)
                    end_time = time.time()
                    times.append(end_time - start_time)
                
                avg_time = statistics.mean(times)
                throughput = batch_size / avg_time  # å›¾åƒ/ç§’
                
                results[device][batch_size] = {
                    'avg_time': avg_time,
                    'throughput': throughput,
                    'times': times
                }
                
                print(f"    å¹³å‡æ—¶é—´: {avg_time:.3f}s, ååé‡: {throughput:.2f} å›¾åƒ/ç§’")
            
            # æ¢å¤è®¾å¤‡è®¾ç½®
            config.device = original_device
            
        self.results['feature_extraction'] = results
        return results
        
    def test_distributed_vs_local(self):
        """æµ‹è¯•åˆ†å¸ƒå¼ vs æœ¬åœ°è®¡ç®—æ€§èƒ½"""
        print("\n" + "="*80)
        print("ğŸŒ åˆ†å¸ƒå¼ vs æœ¬åœ°è®¡ç®—æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        results = {'local': {}, 'distributed': {}}
        
        # æµ‹è¯•å›¾åƒæ•°é‡
        test_counts = [1, 5, 10, 20]
        
        for count in test_counts:
            if count > len(self.test_images):
                continue
                
            print(f"\nğŸ“‹ æµ‹è¯•å›¾åƒæ•°é‡: {count}")
            test_batch = self.test_images[:count]
            
            # æœ¬åœ°è®¡ç®—æµ‹è¯•
            print("  ğŸ  æœ¬åœ°è®¡ç®—æµ‹è¯•...")
            config.DISTRIBUTED_AVAILABLE = False
            
            local_times = []
            for _ in range(3):
                start_time = time.time()
                for img_data in test_batch:
                    features = feature_extractor([img_data['image']])
                end_time = time.time()
                local_times.append(end_time - start_time)
            
            local_avg = statistics.mean(local_times)
            local_throughput = count / local_avg
            
            results['local'][count] = {
                'avg_time': local_avg,
                'throughput': local_throughput,
                'times': local_times
            }
            
            print(f"    å¹³å‡æ—¶é—´: {local_avg:.3f}s, ååé‡: {local_throughput:.2f} å›¾åƒ/ç§’")
            
            # åˆ†å¸ƒå¼è®¡ç®—æµ‹è¯•
            print("  â˜ï¸  åˆ†å¸ƒå¼è®¡ç®—æµ‹è¯•...")
            config.DISTRIBUTED_AVAILABLE = True
            
            try:
                from backend.worker import generate_embeddings_task
                
                distributed_times = []
                for _ in range(3):
                    start_time = time.time()
                    futures = []
                    
                    for img_data in test_batch:
                        future = generate_embeddings_task.delay(img_data['base64'])
                        futures.append(future)
                    
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    for future in futures:
                        result = future.get(timeout=30)
                    
                    end_time = time.time()
                    distributed_times.append(end_time - start_time)
                
                distributed_avg = statistics.mean(distributed_times)
                distributed_throughput = count / distributed_avg
                
                results['distributed'][count] = {
                    'avg_time': distributed_avg,
                    'throughput': distributed_throughput,
                    'times': distributed_times
                }
                
                print(f"    å¹³å‡æ—¶é—´: {distributed_avg:.3f}s, ååé‡: {distributed_throughput:.2f} å›¾åƒ/ç§’")
                
                # è®¡ç®—æ€§èƒ½æ¯”è¾ƒ
                speedup = local_avg / distributed_avg if distributed_avg > 0 else 0
                print(f"    åŠ é€Ÿæ¯”: {speedup:.2f}x")
                
            except Exception as e:
                print(f"    âŒ åˆ†å¸ƒå¼æµ‹è¯•å¤±è´¥: {e}")
                results['distributed'][count] = {'error': str(e)}
        
        self.results['distributed_vs_local'] = results
        return results
        
    def test_faiss_cluster_performance(self):
        """æµ‹è¯•FAISSç°‡å¤§å°å¯¹æ£€ç´¢æ€§èƒ½çš„å½±å“"""
        print("\n" + "="*80)
        print("ğŸ” FAISSç°‡å¤§å°æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        # ä¸åŒçš„ç°‡å¤§å°è®¾ç½®
        n_list_values = [1, 5, 10, 20, 50, 100]
        results = {}
        
        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰æ•°æ®é›†
        try:
            dataset_rows = query_multi("datasets", columns="id, name", limit=1)
            if not dataset_rows:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°æ•°æ®é›†ï¼Œè·³è¿‡FAISSæµ‹è¯•")
                return {}
                
            dataset_id = dataset_rows[0][0]
            print(f"ä½¿ç”¨æ•°æ®é›†ID: {dataset_id}")
            
            # è·å–æ•°æ®é›†ç‰¹å¾
            from backend.search_module.search import get_features_and_ids
            features, ids, image_paths, descriptions = get_features_and_ids(dataset_id)
            
            if features is None or len(features) < 10:
                print("âŒ æ•°æ®é›†ç‰¹å¾å¤ªå°‘ï¼Œè·³è¿‡FAISSæµ‹è¯•")
                return {}
                
            print(f"æ•°æ®é›†åŒ…å« {len(features)} ä¸ªç‰¹å¾å‘é‡")
            
        except Exception as e:
            print(f"âŒ è·å–æ•°æ®é›†å¤±è´¥: {e}")
            return {}
        
        original_n_list = config.N_LIST
        
        for n_list in n_list_values:
            if n_list > len(features) // 2:  # ç°‡æ•°é‡ä¸èƒ½å¤ªå¤§
                continue
                
            print(f"\nğŸ”§ æµ‹è¯•ç°‡å¤§å°: {n_list}")
            config.N_LIST = n_list
            
            try:
                # æµ‹è¯•æŸ¥è¯¢æ€§èƒ½
                query_times = []
                accuracy_scores = []
                
                # ä½¿ç”¨å‰å‡ ä¸ªç‰¹å¾ä½œä¸ºæŸ¥è¯¢
                query_features = features[:min(5, len(features))]
                
                for query_feature in query_features:
                    start_time = time.time()
                    
                    # æ‰§è¡Œæœç´¢
                    similar_indices, distances = search_index(
                        dataset_id, query_feature.reshape(1, -1), k=10
                    )
                    
                    end_time = time.time()
                    query_times.append(end_time - start_time)
                    
                    # ç®€å•çš„å‡†ç¡®æ€§è¯„ä¼°ï¼ˆæ£€æŸ¥æ˜¯å¦æ‰¾åˆ°ç›¸ä¼¼çš„ç»“æœï¼‰
                    if len(similar_indices) > 0:
                        accuracy_scores.append(1.0)
                    else:
                        accuracy_scores.append(0.0)
                
                avg_query_time = statistics.mean(query_times)
                avg_accuracy = statistics.mean(accuracy_scores)
                
                results[n_list] = {
                    'avg_query_time': avg_query_time,
                    'avg_accuracy': avg_accuracy,
                    'query_times': query_times,
                    'throughput': 1.0 / avg_query_time if avg_query_time > 0 else 0
                }
                
                print(f"    å¹³å‡æŸ¥è¯¢æ—¶é—´: {avg_query_time:.4f}s")
                print(f"    æŸ¥è¯¢ååé‡: {results[n_list]['throughput']:.2f} æŸ¥è¯¢/ç§’")
                print(f"    å¹³å‡å‡†ç¡®ç‡: {avg_accuracy:.2f}")
                
            except Exception as e:
                print(f"    âŒ æµ‹è¯•å¤±è´¥: {e}")
                results[n_list] = {'error': str(e)}
        
        # æ¢å¤åŸå§‹é…ç½®
        config.N_LIST = original_n_list
        
        self.results['faiss_cluster'] = results
        return results
        
    def test_concurrent_performance(self):
        """æµ‹è¯•å¹¶å‘å¤„ç†æ€§èƒ½"""
        print("\n" + "="*80)
        print("âš¡ å¹¶å‘å¤„ç†æ€§èƒ½æµ‹è¯•")
        print("="*80)
        
        concurrent_levels = [1, 2, 4, 8]
        results = {}
        
        for level in concurrent_levels:
            print(f"\nğŸ”§ å¹¶å‘ç­‰çº§: {level}")
            
            test_batch = self.test_images[:min(level * 2, len(self.test_images))]
            
            times = []
            for _ in range(3):
                start_time = time.time()
                
                with ThreadPoolExecutor(max_workers=level) as executor:
                    futures = []
                    for img_data in test_batch:
                        future = executor.submit(feature_extractor, [img_data['image']])
                        futures.append(future)
                    
                    # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
                    for future in as_completed(futures):
                        result = future.result()
                
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = statistics.mean(times)
            throughput = len(test_batch) / avg_time
            
            results[level] = {
                'avg_time': avg_time,
                'throughput': throughput,
                'times': times,
                'images_processed': len(test_batch)
            }
            print(f"    å¹³å‡æ—¶é—´: {avg_time:.3f}s")
            print(f"    ååé‡: {throughput:.2f} å›¾åƒ/ç§’")
        
        self.results['concurrent'] = results
        return results
        
    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print("\n" + "="*80)
        print("ğŸ“‹ ç”Ÿæˆæ€§èƒ½æµ‹è¯•æŠ¥å‘Š")
        print("="*80)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_dir = os.path.join("..", "performance_reports", timestamp)
        os.makedirs(report_dir, exist_ok=True)
        
        # ä¿å­˜åŸå§‹æ•°æ®
        with open(os.path.join(report_dir, "raw_results.json"), "w", encoding='utf-8') as f:
            json.dump(self.results, f, indent=2, ensure_ascii=False)
        
        # ç”Ÿæˆæ–‡æœ¬æŠ¥å‘Š
        report_path = os.path.join(report_dir, "performance_report.md")
        with open(report_path, "w", encoding='utf-8') as f:
            f.write(f"# æ€§èƒ½æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            # é…ç½®ä¿¡æ¯
            f.write("## æµ‹è¯•é…ç½®\n\n")
            f.write("| é…ç½®é¡¹ | å€¼ |\n")
            f.write("|--------|----|\n")
            for key, value in self.original_config.items():
                f.write(f"| {key} | {value} |\n")
            f.write("\n")
            
            # ç‰¹å¾æå–æ€§èƒ½
            if 'feature_extraction' in self.results:
                f.write("## ç‰¹å¾æå–æ€§èƒ½\n\n")
                for device, device_results in self.results['feature_extraction'].items():
                    f.write(f"### è®¾å¤‡: {device}\n\n")
                    f.write("| æ‰¹å¤„ç†å¤§å° | å¹³å‡æ—¶é—´(s) | ååé‡(å›¾åƒ/s) |\n")
                    f.write("|------------|-------------|----------------|\n")
                    for batch_size, metrics in device_results.items():
                        f.write(f"| {batch_size} | {metrics['avg_time']:.3f} | {metrics['throughput']:.2f} |\n")
                    f.write("\n")
            
            # åˆ†å¸ƒå¼ vs æœ¬åœ°
            if 'distributed_vs_local' in self.results:
                f.write("## åˆ†å¸ƒå¼ vs æœ¬åœ°è®¡ç®—æ€§èƒ½\n\n")
                f.write("| å›¾åƒæ•°é‡ | æœ¬åœ°æ—¶é—´(s) | åˆ†å¸ƒå¼æ—¶é—´(s) | æœ¬åœ°ååé‡ | åˆ†å¸ƒå¼ååé‡ | åŠ é€Ÿæ¯” |\n")
                f.write("|----------|-------------|---------------|------------|--------------|--------|\n")
                
                local_results = self.results['distributed_vs_local']['local']
                distributed_results = self.results['distributed_vs_local']['distributed']
                
                for count in local_results.keys():
                    local_time = local_results[count]['avg_time']
                    local_throughput = local_results[count]['throughput']
                    
                    if count in distributed_results and 'error' not in distributed_results[count]:
                        dist_time = distributed_results[count]['avg_time']
                        dist_throughput = distributed_results[count]['throughput']
                        speedup = local_time / dist_time if dist_time > 0 else 0
                        f.write(f"| {count} | {local_time:.3f} | {dist_time:.3f} | {local_throughput:.2f} | {dist_throughput:.2f} | {speedup:.2f}x |\n")
                    else:
                        f.write(f"| {count} | {local_time:.3f} | å¤±è´¥ | {local_throughput:.2f} | N/A | N/A |\n")
                f.write("\n")
            
            # FAISSç°‡å¤§å°æ€§èƒ½
            if 'faiss_cluster' in self.results:
                f.write("## FAISSç°‡å¤§å°æ€§èƒ½\n\n")
                f.write("| ç°‡å¤§å° | å¹³å‡æŸ¥è¯¢æ—¶é—´(s) | æŸ¥è¯¢ååé‡(æŸ¥è¯¢/s) | å‡†ç¡®ç‡ |\n")
                f.write("|--------|-----------------|-------------------|--------|\n")
                for n_list, metrics in self.results['faiss_cluster'].items():
                    if 'error' not in metrics:
                        f.write(f"| {n_list} | {metrics['avg_query_time']:.4f} | {metrics['throughput']:.2f} | {metrics['avg_accuracy']:.2f} |\n")
                    else:
                        f.write(f"| {n_list} | å¤±è´¥ | N/A | N/A |\n")
                f.write("\n")
            
            # å¹¶å‘æ€§èƒ½
            if 'concurrent' in self.results:
                f.write("## å¹¶å‘å¤„ç†æ€§èƒ½\n\n")
                f.write("| å¹¶å‘ç­‰çº§ | å¤„ç†å›¾åƒæ•° | å¹³å‡æ—¶é—´(s) | ååé‡(å›¾åƒ/s) |\n")
                f.write("|----------|------------|-------------|----------------|\n")
                for level, metrics in self.results['concurrent'].items():
                    f.write(f"| {level} | {metrics['images_processed']} | {metrics['avg_time']:.3f} | {metrics['throughput']:.2f} |\n")
                f.write("\n")
        
        # ç”Ÿæˆå›¾è¡¨
        self.generate_charts(report_dir)
        
        print(f"âœ… æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}")
        return report_path
        
    def generate_charts(self, report_dir):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        try:
            plt.style.use('seaborn-v0_8')
        except:
            plt.style.use('default')
        
        # ç‰¹å¾æå–æ€§èƒ½å›¾è¡¨
        if 'feature_extraction' in self.results:
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            for device, device_results in self.results['feature_extraction'].items():
                batch_sizes = list(device_results.keys())
                times = [device_results[bs]['avg_time'] for bs in batch_sizes]
                throughputs = [device_results[bs]['throughput'] for bs in batch_sizes]
                
                axes[0].plot(batch_sizes, times, marker='o', label=f'{device} - æ—¶é—´')
                axes[1].plot(batch_sizes, throughputs, marker='s', label=f'{device} - ååé‡')
            
            axes[0].set_xlabel('æ‰¹å¤„ç†å¤§å°')
            axes[0].set_ylabel('å¹³å‡æ—¶é—´ (s)')
            axes[0].set_title('ç‰¹å¾æå–æ—¶é—´ vs æ‰¹å¤„ç†å¤§å°')
            axes[0].legend()
            axes[0].grid(True)
            
            axes[1].set_xlabel('æ‰¹å¤„ç†å¤§å°')
            axes[1].set_ylabel('ååé‡ (å›¾åƒ/s)')
            axes[1].set_title('ç‰¹å¾æå–ååé‡ vs æ‰¹å¤„ç†å¤§å°')
            axes[1].legend()
            axes[1].grid(True)
            
            plt.tight_layout()
            plt.savefig(os.path.join(report_dir, 'feature_extraction_performance.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # åˆ†å¸ƒå¼ vs æœ¬åœ°æ€§èƒ½å›¾è¡¨
        if 'distributed_vs_local' in self.results:
            local_results = self.results['distributed_vs_local']['local']
            distributed_results = self.results['distributed_vs_local']['distributed']
            
            counts = list(local_results.keys())
            local_times = [local_results[c]['avg_time'] for c in counts]
            local_throughputs = [local_results[c]['throughput'] for c in counts]
            
            dist_times = []
            dist_throughputs = []
            for c in counts:
                if c in distributed_results and 'error' not in distributed_results[c]:
                    dist_times.append(distributed_results[c]['avg_time'])
                    dist_throughputs.append(distributed_results[c]['throughput'])
                else:
                    dist_times.append(None)
                    dist_throughputs.append(None)
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            axes[0].plot(counts, local_times, marker='o', label='æœ¬åœ°è®¡ç®—', linewidth=2)
            if any(t is not None for t in dist_times):
                valid_counts = [c for c, t in zip(counts, dist_times) if t is not None]
                valid_times = [t for t in dist_times if t is not None]
                axes[0].plot(valid_counts, valid_times, marker='s', label='åˆ†å¸ƒå¼è®¡ç®—', linewidth=2)
            
            axes[0].set_xlabel('å›¾åƒæ•°é‡')
            axes[0].set_ylabel('å¹³å‡æ—¶é—´ (s)')
            axes[0].set_title('å¤„ç†æ—¶é—´å¯¹æ¯”')
            axes[0].legend()
            axes[0].grid(True)
            
            axes[1].plot(counts, local_throughputs, marker='o', label='æœ¬åœ°è®¡ç®—', linewidth=2)
            if any(t is not None for t in dist_throughputs):
                valid_counts = [c for c, t in zip(counts, dist_throughputs) if t is not None]
                valid_throughputs = [t for t in dist_throughputs if t is not None]
                axes[1].plot(valid_counts, valid_throughputs, marker='s', label='åˆ†å¸ƒå¼è®¡ç®—', linewidth=2)
            
            axes[1].set_xlabel('å›¾åƒæ•°é‡')
            axes[1].set_ylabel('ååé‡ (å›¾åƒ/s)')
            axes[1].set_title('ååé‡å¯¹æ¯”')
            axes[1].legend()
            axes[1].grid(True)
            
            plt.tight_layout()
            plt.savefig(os.path.join(report_dir, 'distributed_vs_local.png'), dpi=300, bbox_inches='tight')
            plt.close()
        
        # FAISSç°‡å¤§å°æ€§èƒ½å›¾è¡¨
        if 'faiss_cluster' in self.results:
            valid_results = {k: v for k, v in self.results['faiss_cluster'].items() if 'error' not in v}
            
            if valid_results:
                n_lists = list(valid_results.keys())
                query_times = [valid_results[n]['avg_query_time'] for n in n_lists]
                throughputs = [valid_results[n]['throughput'] for n in n_lists]
                accuracies = [valid_results[n]['avg_accuracy'] for n in n_lists]
                
                fig, axes = plt.subplots(1, 3, figsize=(18, 6))
                
                axes[0].plot(n_lists, query_times, marker='o', linewidth=2, color='red')
                axes[0].set_xlabel('ç°‡å¤§å° (N_LIST)')
                axes[0].set_ylabel('å¹³å‡æŸ¥è¯¢æ—¶é—´ (s)')
                axes[0].set_title('æŸ¥è¯¢æ—¶é—´ vs ç°‡å¤§å°')
                axes[0].grid(True)
                
                axes[1].plot(n_lists, throughputs, marker='s', linewidth=2, color='blue')
                axes[1].set_xlabel('ç°‡å¤§å° (N_LIST)')
                axes[1].set_ylabel('æŸ¥è¯¢ååé‡ (æŸ¥è¯¢/s)')
                axes[1].set_title('æŸ¥è¯¢ååé‡ vs ç°‡å¤§å°')
                axes[1].grid(True)
                
                axes[2].plot(n_lists, accuracies, marker='^', linewidth=2, color='green')
                axes[2].set_xlabel('ç°‡å¤§å° (N_LIST)')
                axes[2].set_ylabel('å‡†ç¡®ç‡')
                axes[2].set_title('å‡†ç¡®ç‡ vs ç°‡å¤§å°')
                axes[2].grid(True)
                
                plt.tight_layout()
                plt.savefig(os.path.join(report_dir, 'faiss_cluster_performance.png'), dpi=300, bbox_inches='tight')
                plt.close()
        
        print("âœ… æ€§èƒ½å›¾è¡¨å·²ç”Ÿæˆ")
        
    def run_full_benchmark(self):
        """è¿è¡Œå®Œæ•´çš„æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        print("ğŸš€ å¼€å§‹è¿è¡Œå®Œæ•´æ€§èƒ½åŸºå‡†æµ‹è¯•")
        print("=" * 80)
        
        start_time = time.time()
        
        try:
            # ç”Ÿæˆæµ‹è¯•å›¾åƒ
            self.generate_test_images(count=30)
            
            # è¿è¡Œå„é¡¹æµ‹è¯•
            self.test_feature_extraction_performance()
            self.test_distributed_vs_local()
            self.test_faiss_cluster_performance()
            self.test_concurrent_performance()
            
            # ç”ŸæˆæŠ¥å‘Š
            report_path = self.generate_report()
            
            total_time = time.time() - start_time
            print(f"\nâœ… å®Œæ•´åŸºå‡†æµ‹è¯•å®Œæˆï¼Œæ€»è€—æ—¶: {total_time:.2f}ç§’")
            print(f"ğŸ“„ æŠ¥å‘Šè·¯å¾„: {report_path}")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            import traceback
            traceback.print_exc()
        finally:
            # æ¢å¤åŸå§‹é…ç½®
            self.restore_config()
            
    def run_custom_test(self, test_configs):
        """è¿è¡Œè‡ªå®šä¹‰é…ç½®æµ‹è¯•"""
        print("ğŸ”§ è¿è¡Œè‡ªå®šä¹‰é…ç½®æµ‹è¯•")
        print("=" * 80)
        
        results = {}
        
        for config_name, test_config in test_configs.items():
            print(f"\nğŸ“Š æµ‹è¯•é…ç½®: {config_name}")
            print("-" * 40)
            
            # åº”ç”¨é…ç½®
            for key, value in test_config.items():
                if hasattr(config, key):
                    setattr(config, key, value)
                    print(f"  {key}: {value}")
            
            # è¿è¡Œç‰¹å¾æå–æµ‹è¯•
            if not hasattr(self, 'test_images') or not self.test_images:
                self.generate_test_images(count=10)
            
            # ç®€å•çš„æ€§èƒ½æµ‹è¯•
            test_batch = self.test_images[:5]
            times = []
            
            for _ in range(3):
                start_time = time.time()
                for img_data in test_batch:
                    features = feature_extractor([img_data['image']])
                end_time = time.time()
                times.append(end_time - start_time)
            
            avg_time = statistics.mean(times)
            throughput = len(test_batch) / avg_time
            
            results[config_name] = {
                'config': test_config,
                'avg_time': avg_time,
                'throughput': throughput,
                'times': times
            }
            
            print(f"  ç»“æœ: å¹³å‡æ—¶é—´ {avg_time:.3f}s, ååé‡ {throughput:.2f} å›¾åƒ/ç§’")
        
        # æ¢å¤åŸå§‹é…ç½®
        self.restore_config()
        
        return results


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='æ€§èƒ½æµ‹è¯•åŸºå‡†å·¥å…·')
    parser.add_argument('--mode', choices=['full', 'quick', 'custom'], default='full',
                      help='æµ‹è¯•æ¨¡å¼: full(å®Œæ•´æµ‹è¯•), quick(å¿«é€Ÿæµ‹è¯•), custom(è‡ªå®šä¹‰æµ‹è¯•)')
    parser.add_argument('--images', type=int, default=20,
                      help='æµ‹è¯•å›¾åƒæ•°é‡ (é»˜è®¤: 20)')
    
    args = parser.parse_args()
    
    testbench = PerformanceTestbench()
    
    if args.mode == 'full':
        testbench.run_full_benchmark()
        
    elif args.mode == 'quick':
        print("ğŸƒ å¿«é€Ÿæ€§èƒ½æµ‹è¯•")
        testbench.generate_test_images(count=min(args.images, 10))
        testbench.test_feature_extraction_performance()
        testbench.generate_report()
        
    elif args.mode == 'custom':
        # ç¤ºä¾‹è‡ªå®šä¹‰é…ç½®
        custom_configs = {
            'cpu_small_batch': {
                'device': 'cpu',
                'batchsize': 4,
                'N_LIST': 5
            },
            'cpu_large_batch': {
                'device': 'cpu',
                'batchsize': 32,
                'N_LIST': 10
            }
        }
        
        # å¦‚æœæ”¯æŒGPUï¼Œæ·»åŠ GPUé…ç½®
        try:
            import torch
            if torch.cuda.is_available():
                custom_configs.update({
                    'gpu_small_batch': {
                        'device': 'cuda',
                        'batchsize': 8,
                        'N_LIST': 5
                    },
                    'gpu_large_batch': {
                        'device': 'cuda',
                        'batchsize': 64,
                        'N_LIST': 20
                    }
                })
        except ImportError:
            pass
        
        testbench.generate_test_images(count=args.images)
        results = testbench.run_custom_test(custom_configs)
        
        # æ‰“å°ç»“æœå¯¹æ¯”
        print("\nğŸ“Š è‡ªå®šä¹‰é…ç½®æµ‹è¯•ç»“æœå¯¹æ¯”")
        print("=" * 80)
        
        headers = ['é…ç½®åç§°', 'å¹³å‡æ—¶é—´(s)', 'ååé‡(å›¾åƒ/s)', 'è®¾å¤‡', 'æ‰¹å¤„ç†å¤§å°', 'ç°‡å¤§å°']
        table_data = []
        
        for config_name, result in results.items():
            row = [
                config_name,
                f"{result['avg_time']:.3f}",
                f"{result['throughput']:.2f}",
                result['config'].get('device', 'N/A'),
                result['config'].get('batchsize', 'N/A'),
                result['config'].get('N_LIST', 'N/A')
            ]
            table_data.append(row)
        
        print(tabulate(table_data, headers=headers, tablefmt='grid'))


if __name__ == "__main__":
    main()
